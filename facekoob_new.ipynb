{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "facekoob-new",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Zonglin-Li6565/smiley-face/blob/master/facekoob_new.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "KkeUHJM4k6Aj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!/opt/bin/nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p3jyCajDl8E7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!free -m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NXXoi7ncrF-d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import dlib\n",
        "except ImportError as error:\n",
        "  !apt update\n",
        "  !apt install -y cmake\n",
        "  !pip install dlib\n",
        "  import dlib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PvmyCimQz3KT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import cv2\n",
        "\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oocFH72IVSbJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "try:\n",
        "  import torch\n",
        "except ImportError as error:\n",
        "  from os import path\n",
        "  from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "  platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "  accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "  !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "  import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZLJ1HMpoZb3w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Module, functional"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CF208Goo5glr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Get the dataset and opencv classifier xml"
      ]
    },
    {
      "metadata": {
        "id": "0mecELblj69c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if not os.path.isfile('lfw.tgz'):\n",
        "  !wget http://vis-www.cs.umass.edu/lfw/lfw.tgz\n",
        "  !tar xzf lfw.tgz\n",
        "  !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "  !bzip2 -d shape_predictor_68_face_landmarks.dat.bz2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "noYHE3Ya1cCb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dG7fsYZKvNiO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!lscpu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NgYnjO1p2EjZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Preprocess the faces\n",
        "Will find the face, crop it and set a file flag when done"
      ]
    },
    {
      "metadata": {
        "id": "0fwSsmy8tCgX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Copied from https://github.com/cmusatyalab/openface/blob/master/openface/align_dlib.py\n",
        "TEMPLATE = np.float32([\n",
        "    (0.0792396913815, 0.339223741112), (0.0829219487236, 0.456955367943),\n",
        "    (0.0967927109165, 0.575648016728), (0.122141515615, 0.691921601066),\n",
        "    (0.168687863544, 0.800341263616), (0.239789390707, 0.895732504778),\n",
        "    (0.325662452515, 0.977068762493), (0.422318282013, 1.04329000149),\n",
        "    (0.531777802068, 1.06080371126), (0.641296298053, 1.03981924107),\n",
        "    (0.738105872266, 0.972268833998), (0.824444363295, 0.889624082279),\n",
        "    (0.894792677532, 0.792494155836), (0.939395486253, 0.681546643421),\n",
        "    (0.96111933829, 0.562238253072), (0.970579841181, 0.441758925744),\n",
        "    (0.971193274221, 0.322118743967), (0.163846223133, 0.249151738053),\n",
        "    (0.21780354657, 0.204255863861), (0.291299351124, 0.192367318323),\n",
        "    (0.367460241458, 0.203582210627), (0.4392945113, 0.233135599851),\n",
        "    (0.586445962425, 0.228141644834), (0.660152671635, 0.195923841854),\n",
        "    (0.737466449096, 0.182360984545), (0.813236546239, 0.192828009114),\n",
        "    (0.8707571886, 0.235293377042), (0.51534533827, 0.31863546193),\n",
        "    (0.516221448289, 0.396200446263), (0.517118861835, 0.473797687758),\n",
        "    (0.51816430343, 0.553157797772), (0.433701156035, 0.604054457668),\n",
        "    (0.475501237769, 0.62076344024), (0.520712933176, 0.634268222208),\n",
        "    (0.565874114041, 0.618796581487), (0.607054002672, 0.60157671656),\n",
        "    (0.252418718401, 0.331052263829), (0.298663015648, 0.302646354002),\n",
        "    (0.355749724218, 0.303020650651), (0.403718978315, 0.33867711083),\n",
        "    (0.352507175597, 0.349987615384), (0.296791759886, 0.350478978225),\n",
        "    (0.631326076346, 0.334136672344), (0.679073381078, 0.29645404267),\n",
        "    (0.73597236153, 0.294721285802), (0.782865376271, 0.321305281656),\n",
        "    (0.740312274764, 0.341849376713), (0.68499850091, 0.343734332172),\n",
        "    (0.353167761422, 0.746189164237), (0.414587777921, 0.719053835073),\n",
        "    (0.477677654595, 0.706835892494), (0.522732900812, 0.717092275768),\n",
        "    (0.569832064287, 0.705414478982), (0.635195811927, 0.71565572516),\n",
        "    (0.69951672331, 0.739419187253), (0.639447159575, 0.805236879972),\n",
        "    (0.576410514055, 0.835436670169), (0.525398405766, 0.841706377792),\n",
        "    (0.47641545769, 0.837505914975), (0.41379548902, 0.810045601727),\n",
        "    (0.380084785646, 0.749979603086), (0.477955996282, 0.74513234612),\n",
        "    (0.523389793327, 0.748924302636), (0.571057789237, 0.74332894691),\n",
        "    (0.672409137852, 0.744177032192), (0.572539621444, 0.776609286626),\n",
        "    (0.5240106503, 0.783370783245), (0.477561227414, 0.778476346951)])\n",
        "\n",
        "INV_TEMPLATE = np.float32([\n",
        "    (-0.04099179660567834, -0.008425234314031194, 2.575498465013183),\n",
        "    (0.04062510634554352, -0.009678089746831375, -1.2534351452524177),\n",
        "    (0.0003666902601348179, 0.01810332406086298, -0.32206331976076663)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D11wDYxGtGL0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TPL_MIN, TPL_MAX = np.min(TEMPLATE, axis=0), np.max(TEMPLATE, axis=0)\n",
        "MINMAX_TEMPLATE = (TEMPLATE - TPL_MIN) / (TPL_MAX - TPL_MIN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4YSwK6K5tIkw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "INNER_EYES_AND_BOTTOM_LIP = [39, 42, 57]\n",
        "OUTER_EYES_AND_NOSE = [36, 45, 33]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IKVtBnYBtN6-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CropAndAlign:\n",
        "    def __init__(self, face_predictor_path, landmark_indices, logger):\n",
        "        self.detector = dlib.get_frontal_face_detector()\n",
        "        self.land_mark_predictor = dlib.shape_predictor(face_predictor_path)\n",
        "        self.land_mark_indices = np.array(landmark_indices)\n",
        "        self.logger = logger\n",
        "\n",
        "    def find_all_bounding_boxes(self, rgb_img):\n",
        "        try:\n",
        "            # Upsample the image once\n",
        "            return self.detector(rgb_img, 1)\n",
        "        except Exception as e:\n",
        "            self.logger.warn(e)\n",
        "            return []\n",
        "\n",
        "    def get_largest_bounding_box(self, rgb_img):\n",
        "        faces = self.find_all_bounding_boxes(rgb_img)\n",
        "        if len(faces) > 0:\n",
        "            return max(faces, key=lambda box: box.width() * box.height())\n",
        "        else:\n",
        "            self.logger.warn('No face was found in image')\n",
        "            return None\n",
        "\n",
        "    def find_landmarks(self, rgb_img, box):\n",
        "        points = self.land_mark_predictor(rgb_img, box)\n",
        "        return np.float32(list(map(lambda p: (p.x, p.y), points.parts())))\n",
        "\n",
        "    def align_one_face(self, rgb_img, box, out_size):\n",
        "        landmarks = self.find_landmarks(rgb_img, box)\n",
        "        affine_transform = cv2.getAffineTransform(\n",
        "            landmarks[self.land_mark_indices],\n",
        "            out_size * MINMAX_TEMPLATE[self.land_mark_indices])\n",
        "        return cv2.warpAffine(rgb_img, affine_transform, (out_size, out_size))\n",
        "\n",
        "    def align_biggest_face(self, rgb_img, out_size):\n",
        "        box = self.get_largest_bounding_box(rgb_img)\n",
        "        if box is not None:\n",
        "            return self.align_one_face(rgb_img, box, out_size)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def align_all_faces(self, rgb_img, out_size):\n",
        "        boxes = self.find_all_bounding_boxes(rgb_img)\n",
        "        return [self.align_one_face(rgb_img, box, out_size) for box in boxes]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2A8FZg1ntP6O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_image_(aligner, in_path, out_path, dim):\n",
        "    logger.debug('Processing %s' % in_path)\n",
        "    image = cv2.imread(in_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    if image is None:\n",
        "        logger.error('Failed to load image %s' % in_path)\n",
        "        return\n",
        "    aligned = aligner.align_biggest_face(image, dim)\n",
        "    if aligned is None:\n",
        "        logger.warning('No face found in %s' % in_path)\n",
        "    else:\n",
        "        aligned = cv2.cvtColor(aligned, cv2.COLOR_BGR2RGB)\n",
        "        cv2.imwrite(out_path, aligned)\n",
        "        del aligned"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ndQM1vktSyV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(input_dir,\n",
        "                       output_dir,\n",
        "                       out_dim,\n",
        "                       face_predictor_path,\n",
        "                       logger,\n",
        "                       landmark_indices=INNER_EYES_AND_BOTTOM_LIP):\n",
        "    if os.path.exists(output_dir):\n",
        "        logger.error('Output dir %s exists' % output_dir)\n",
        "        return\n",
        "    if not os.path.exists(input_dir):\n",
        "        logger.error('Data dir %s doesn\\'t exist' % input_dir)\n",
        "        return\n",
        "    elif not os.path.exists(face_predictor_path):\n",
        "        logger.error('Predictor %s doesn\\'t exist' % face_predictor_path)\n",
        "        return\n",
        "\n",
        "    aligner = CropAndAlign(face_predictor_path, landmark_indices, logger)\n",
        "\n",
        "    global global_aligner\n",
        "    global_aligner = aligner\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for image_dir in os.listdir(input_dir):\n",
        "        image_output_dir = os.path.join(\n",
        "            output_dir, os.path.basename(os.path.basename(image_dir)))\n",
        "        if not os.path.exists(image_output_dir):\n",
        "            os.makedirs(image_output_dir)\n",
        "\n",
        "    image_paths = glob.glob(os.path.join(input_dir, '**/*.jpg'))\n",
        "\n",
        "    for image_path in image_paths:\n",
        "        image_output_dir = os.path.join(\n",
        "            output_dir, os.path.basename(os.path.dirname(image_path)))\n",
        "        output_path = os.path.join(image_output_dir,\n",
        "                                   os.path.basename(image_path))\n",
        "        process_image_(aligner, image_path, output_path, out_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d6kDDVZGtVcl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO, \n",
        "                    format='%(asctime)s;%(levelname)s;%(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "preprocess_dataset('lfw', 'processed', 72, \n",
        "                   'shape_predictor_68_face_landmarks.dat', logger)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M0U5aHA0iO5K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define the model"
      ]
    },
    {
      "metadata": {
        "id": "-u7GWY83iR5T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Embedder(Module):\n",
        "    def __init__(self, input_size, kernel_sizes):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=kernel_sizes[0])\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=kernel_sizes[1], stride=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=kernel_sizes[2])\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=kernel_sizes[3], stride=1)\n",
        "        self.conv3 = nn.Conv2d(128, 64, kernel_size=kernel_sizes[4])\n",
        "        self.pool3 = nn.AvgPool2d(kernel_size=kernel_sizes[5], stride=1)\n",
        "        self.conv4 = nn.Conv2d(64, 32, kernel_size=kernel_sizes[6])\n",
        "        self.pool4 = nn.AvgPool2d(kernel_size=kernel_sizes[7], stride=1)\n",
        "        size_reduction = sum(kernel_sizes) - len(kernel_sizes)\n",
        "        self.fc_input_dimension = ((input_size[0] - size_reduction) *\n",
        "                                   (input_size[1] - size_reduction) * 32)\n",
        "        self.fc1 = nn.Linear(self.fc_input_dimension, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = Variable(x, requires_grad=False)\n",
        "        x = functional.relu(self.pool1(self.conv1(x)))\n",
        "        x = functional.dropout2d(x, p=0.5)\n",
        "        x = functional.relu(self.pool2(self.conv2(x)))\n",
        "        x = functional.dropout2d(x, p=0.2)\n",
        "        x = functional.relu(self.pool3(self.conv3(x)))\n",
        "        x = functional.dropout2d(x, p=0.1)\n",
        "        x = functional.relu(self.pool4(self.conv4(x)))\n",
        "        x = x.view(-1, self.fc_input_dimension)\n",
        "        x = functional.relu(self.fc1(x))\n",
        "        x = functional.sigmoid(self.fc2(x))\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MhkO7HDOrALJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ]
    },
    {
      "metadata": {
        "id": "PyoKQeFtz4C-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0hx5SkkYuJYZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DataGenerator:\n",
        "    def __init__(self, data_dir, batch_size, logger):\n",
        "        self.data_dir = data_dir\n",
        "        self.face_images = []\n",
        "        if not os.path.exists(data_dir):\n",
        "            logger.error('Data dir %s doesn\\'t exist' % data_dir)\n",
        "            return\n",
        "        people = os.listdir(data_dir)\n",
        "        for person in people:\n",
        "            person_dir = os.path.join(data_dir, person)\n",
        "            faces = os.listdir(person_dir)\n",
        "            if len(faces) > 0:\n",
        "                face_images = list(\n",
        "                    map(lambda img_name: os.path.join(person_dir, img_name),\n",
        "                        faces))\n",
        "                self.face_images.append(face_images)\n",
        "        self.batch_size = batch_size\n",
        "        self.logger = logger\n",
        "        self.person_idx = 0\n",
        "        self.image_idx = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        anchors = []\n",
        "        positives = []\n",
        "        negatives = []\n",
        "\n",
        "        batch_counter = 0\n",
        "        while (batch_counter < self.batch_size) and len(self.face_images) > 1:\n",
        "            if self.person_idx == 0:\n",
        "                random.shuffle(self.face_images)\n",
        "                self.person_idx += 1\n",
        "            if self.image_idx == 0:\n",
        "                random.shuffle(self.face_images[self.person_idx])\n",
        "                self.image_idx = 1\n",
        "\n",
        "            if len(self.face_images[self.person_idx]) > 1:\n",
        "                anchors.append(\n",
        "                    cv2.imread(self.face_images[self.person_idx][\n",
        "                        self.image_idx]).astype(np.float))\n",
        "                positives.append(\n",
        "                    cv2.imread(self.face_images[self.person_idx][self.image_idx\n",
        "                                                                 - 1]).astype(\n",
        "                                                                     np.float))\n",
        "                negative_idx = random.randint(\n",
        "                    0,\n",
        "                    len(self.face_images[self.person_idx - 1]) - 1)\n",
        "                negatives.append(\n",
        "                    cv2.imread(self.face_images[self.person_idx -\n",
        "                                                1][negative_idx]).astype(\n",
        "                                                    np.float))\n",
        "                batch_counter += 1\n",
        "\n",
        "            self.image_idx += 1\n",
        "            if self.image_idx >= len(self.face_images[self.person_idx]):\n",
        "                self.image_idx = 0\n",
        "                self.person_idx += 1\n",
        "                self.person_idx %= len(self.face_images)\n",
        "\n",
        "        anchors = torch.Tensor(anchors)\n",
        "        positives = torch.Tensor(positives)\n",
        "        negatives = torch.Tensor(negatives)\n",
        "        batch = torch.cat((anchors, positives, negatives), dim=0)\n",
        "        batch = torch.transpose(batch, 1, 3)\n",
        "        batch = torch.transpose(batch, 2, 3)\n",
        "        return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Us3TeaXHuNd5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LoadingWorker(mp.Process):\n",
        "    def __init__(self, data_dir, batch_size, queue):\n",
        "        super().__init__()\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.generator = DataGenerator(data_dir, batch_size, self.logger)\n",
        "        self.queue = queue\n",
        "        self.exit = mp.Event()\n",
        "\n",
        "    def run(self):\n",
        "        while not self.exit.is_set():\n",
        "            batch = self.generator.__next__()\n",
        "            self.queue.put(batch)\n",
        "        time.sleep(2)\n",
        "\n",
        "    def terminate(self):\n",
        "        self.logger.info('Shutting down loader')\n",
        "        self.exit.set()\n",
        "        while not self.queue.empty():\n",
        "            self.queue.get()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hi_RLfPzuRNl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss_fn(batch_size, embeddings, alpha=0.2):\n",
        "    anchor = embeddings[:batch_size]\n",
        "    positive = embeddings[batch_size:2 * batch_size]\n",
        "    negative = embeddings[2 * batch_size:]\n",
        "    loss = (\n",
        "        (torch.norm(anchor - positive)**2 - torch.norm(anchor - negative)**2) /\n",
        "        batch_size + alpha)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HmzcwB1puUH2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(data_dir,\n",
        "          batch_size,\n",
        "          total_iter,\n",
        "          kernel_sizes,\n",
        "          logger,\n",
        "          model_save_path,\n",
        "          input_shape=(72, 72),\n",
        "          cuda=True):\n",
        "    queue = mp.Queue(10)\n",
        "    process = LoadingWorker(data_dir, batch_size, queue)\n",
        "    process.start()\n",
        "    model = Embedder(input_shape, kernel_sizes)\n",
        "    if cuda:\n",
        "        model = model.cuda()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=5e-6)\n",
        "    for i in range(total_iter):\n",
        "        optimizer.zero_grad()\n",
        "        images = queue.get()\n",
        "        if cuda:\n",
        "            images = images.cuda()\n",
        "        embeddings = model(images)\n",
        "        loss = loss_fn(batch_size, embeddings)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i % 50 == 0:\n",
        "            logger.info('loss at step %04d is: %f' % (i, loss))\n",
        "    process.terminate()\n",
        "    process.join()\n",
        "    torch.save(model.cpu().state_dict(), model_save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TQA3pO09uaDM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DATA_DIR = 'processed/'\n",
        "BATCH_SIZE = 32\n",
        "TOTAL_ITER = 40000\n",
        "KERNEL_SIZES = [5, 3, 5, 3, 3, 3, 3, 3]\n",
        "train(DATA_DIR, BATCH_SIZE, TOTAL_ITER, KERNEL_SIZES, logger,\n",
        "      'model-weights')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QhNfWM2UQ85k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Download the model"
      ]
    },
    {
      "metadata": {
        "id": "sGPW85sYIOv-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "files.download('model-weights')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c3kuk7NjZAdN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}